#!/usr/bin/env python3
"""
Whisper æ¨¡å‹ä¸‹è½½è„šæœ¬
ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„ OpenAI Whisper æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜
"""

import whisper
import os
import sys
from pathlib import Path

# æ‰€æœ‰å¯ç”¨çš„ Whisper æ¨¡å‹
WHISPER_MODELS = [
    "tiny.en",
    "tiny", 
    "base.en",
    "base",
    "small.en", 
    "small",
    "medium.en",
    "medium",
    "large-v1",
    "large-v2", 
    "large-v3",
    "large"
]

def download_model(model_name: str):
    """
    ä¸‹è½½æŒ‡å®šçš„ Whisper æ¨¡å‹
    """
    try:
        print(f"ğŸ”„ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        
        # ä½¿ç”¨ whisper.load_model() ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•
        model = whisper.load_model(model_name)
        
        print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
        
        # é‡Šæ”¾å†…å­˜
        del model
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_name} ä¸‹è½½å¤±è´¥: {e}")
        return False

def get_cache_dir():
    """
    è·å– Whisper æ¨¡å‹ç¼“å­˜ç›®å½•
    """
    cache_dir = os.path.expanduser("~/.cache/whisper")
    return Path(cache_dir)

def list_cached_models():
    """
    åˆ—å‡ºå·²ç¼“å­˜çš„æ¨¡å‹
    """
    cache_dir = get_cache_dir()
    if not cache_dir.exists():
        return []
    
    cached_models = []
    for file in cache_dir.glob("*.pt"):
        model_name = file.stem
        cached_models.append(model_name)
    
    return cached_models

def main():
    print("ğŸš€ Whisper æ¨¡å‹æ‰¹é‡ä¸‹è½½å·¥å…·")
    print("=" * 50)
    
    # æ˜¾ç¤ºç¼“å­˜ç›®å½•
    cache_dir = get_cache_dir()
    print(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {cache_dir}")
    
    # åˆ—å‡ºå·²ç¼“å­˜çš„æ¨¡å‹
    cached_models = list_cached_models()
    if cached_models:
        print(f"ğŸ“¦ å·²ç¼“å­˜çš„æ¨¡å‹: {', '.join(cached_models)}")
    else:
        print("ğŸ“¦ æš‚æ— å·²ç¼“å­˜çš„æ¨¡å‹")
    
    print("\nğŸ¯ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for i, model in enumerate(WHISPER_MODELS, 1):
        status = "âœ… å·²ç¼“å­˜" if model in cached_models else "â³ å¾…ä¸‹è½½"
        print(f"  {i:2d}. {model:12s} - {status}")
    
    print("\n" + "=" * 50)
    
    # è¯¢é—®ç”¨æˆ·é€‰æ‹©
    while True:
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ:\n1. ä¸‹è½½æ‰€æœ‰æ¨¡å‹\n2. ä¸‹è½½æŒ‡å®šæ¨¡å‹\n3. ä¸‹è½½AMD 7900XTXæ¨èæ¨¡å‹ (medium, large-v2, large-v3)\n4. é€€å‡º\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
            print("\nğŸ¯ å¼€å§‹ä¸‹è½½æ‰€æœ‰æ¨¡å‹...")
            success_count = 0
            for model in WHISPER_MODELS:
                if download_model(model):
                    success_count += 1
            print(f"\nğŸ“Š ä¸‹è½½å®Œæˆ! æˆåŠŸ: {success_count}/{len(WHISPER_MODELS)}")
            break
            
        elif choice == "2":
            # ä¸‹è½½æŒ‡å®šæ¨¡å‹
            print("\nå¯ç”¨æ¨¡å‹:")
            for i, model in enumerate(WHISPER_MODELS, 1):
                print(f"  {i}. {model}")
            
            try:
                model_choice = int(input("è¯·è¾“å…¥æ¨¡å‹ç¼–å·: ").strip()) - 1
                if 0 <= model_choice < len(WHISPER_MODELS):
                    model_name = WHISPER_MODELS[model_choice]
                    download_model(model_name)
                else:
                    print("âŒ æ— æ•ˆçš„æ¨¡å‹ç¼–å·")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
            break
            
        elif choice == "3":
            # ä¸‹è½½æ¨èæ¨¡å‹ - é’ˆå¯¹AMD 7900XTXä¼˜åŒ–
            amd_optimized_models = ["medium", "large-v2", "large-v3"]
            print(f"\nğŸ¯ å¼€å§‹ä¸‹è½½AMD 7900XTXæ¨èæ¨¡å‹: {', '.join(amd_optimized_models)}")
            print("   - medium: å¹³è¡¡é€Ÿåº¦ï¼Œé€‚åˆå¿«é€Ÿå¤„ç†")
            print("   - large-v2: å¹³è¡¡æ¨¡å¼ï¼Œå…¼é¡¾è´¨é‡å’Œé€Ÿåº¦") 
            print("   - large-v3: æœ€é«˜è´¨é‡ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¡æ€§èƒ½")
            success_count = 0
            for model in amd_optimized_models:
                if download_model(model):
                    success_count += 1
            print(f"\nğŸ“Š ä¸‹è½½å®Œæˆ! æˆåŠŸ: {success_count}/{len(amd_optimized_models)}")
            break
            
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡ºç¨‹åº")
            sys.exit(0)
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 50)
    cached_models_final = list_cached_models()
    print(f"ğŸ“¦ æœ€ç»ˆç¼“å­˜çš„æ¨¡å‹æ•°é‡: {len(cached_models_final)}")
    if cached_models_final:
        print(f"ğŸ“¦ å·²ç¼“å­˜çš„æ¨¡å‹: {', '.join(sorted(cached_models_final))}")
    
    print(f"ğŸ’¾ æ€»ç¼“å­˜å¤§å°: {get_cache_size()}")
    print("\nâœ… æ¨¡å‹ä¸‹è½½ä»»åŠ¡å®Œæˆ!")

def get_cache_size():
    """
    è®¡ç®—ç¼“å­˜ç›®å½•å¤§å°
    """
    cache_dir = get_cache_dir()
    if not cache_dir.exists():
        return "0 MB"
    
    total_size = 0
    for file in cache_dir.rglob("*"):
        if file.is_file():
            total_size += file.stat().st_size
    
    # è½¬æ¢ä¸º MB
    size_mb = total_size / (1024 * 1024)
    if size_mb < 1024:
        return f"{size_mb:.1f} MB"
    else:
        return f"{size_mb/1024:.1f} GB"

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)

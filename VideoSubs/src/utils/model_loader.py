import os
import whisper
import torch
from typing import Literal

_models = {}  # ç¼“å­˜å¤šä¸ªæ¨¡å‹

ModelSize = Literal['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']

def get_whisper_model(model_size: ModelSize = None):
    """Get Whisper model with dynamic size selection"""
    global _models
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
    if model_size is None:
        model_size = os.environ.get("WHISPER_MODEL", "base")
    
    # å¦‚æœå·²åŠ è½½ï¼Œç›´æ¥è¿”å›
    if model_size in _models:
        return _models[model_size]
    
    # ä¼˜å…ˆä½¿ç”¨ CUDA
    device = "cpu"
    if torch.cuda.is_available():
        try:
            # å°è¯•åœ¨ GPU ä¸Šåˆ†é…ä¸€ä¸ªå°å¼ é‡æ¥éªŒè¯å¯ç”¨æ€§
            # RTX 50 ç³»åˆ— (Blackwell) å¯èƒ½å› ä¸º PyTorch ç‰ˆæœ¬æ»åè€Œå¯¼è‡´ CUDA åˆå§‹åŒ–å¤±è´¥
            t = torch.tensor([1]).cuda()
            # è¿›ä¸€æ­¥éªŒè¯ï¼šæ‰§è¡Œç®€å•è®¡ç®—ï¼Œç¡®ä¿ kernel å¯ç”¨
            (t + 1).cpu()
            device = "cuda"
            device_name = torch.cuda.get_device_name(0)
            print(f"âœ… CUDA is available and working. Using GPU: {device_name}")
            if "5070" in device_name or "5080" in device_name or "5090" in device_name:
                 print("â„¹ï¸  RTX 50-series GPU detected. Ignoring potential PyTorch compatibility warnings if tensor allocation succeeded.")
        except Exception as e:
            print(f"âš ï¸ CUDA is available but failed to initialize (likely architecture mismatch for RTX 50 series).")
            print(f"   Error: {e}")
            print("   Falling back to CPU.")
            device = "cpu"
    else:
        print("âš ï¸ CUDA not available. Using CPU. This will be slow!")
        print("   Please ensure you have installed PyTorch with CUDA support.")
    
    print(f"Loading OpenAI Whisper model: {model_size} on {device}")
    
    try:
        model = whisper.load_model(model_size, device=device)
    except Exception as e:
        if device == "cuda":
            print(f"âŒ Failed to load model on GPU (Kernel Error): {e}")
            print("ğŸ”„ Falling back to CPU...")
            device = "cpu"
            print(f"Loading OpenAI Whisper model: {model_size} on {device}")
            model = whisper.load_model(model_size, device=device)
        else:
            raise e

    print(f"OpenAI Whisper model '{model_size}' loaded successfully")
    
    # ç¼“å­˜æ¨¡å‹
    _models[model_size] = model
    return model
